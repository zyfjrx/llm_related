{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0867b97",
   "metadata": {},
   "source": [
    "## 1. 为什么关注强化学习\n",
    "* 强化学习得到的模型可以有超人类的表现。\n",
    "* 监督学习获取的监督数据，其实是人来标注的，比如 ImageNet 的图片的标签都是人类标注的。因此可以确定监督 学习算法的上限就是人类的表现，标注结果决定了它的表现永远不可能超越人类\n",
    "* 强化学习它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现，比如 DeepMind 的 AlphaGo 这样一个强化学习的算法可以把人类顶尖的棋手打败\n",
    "## 2. 强化学习几个关键概念\n",
    "* 状态\n",
    "* 动作\n",
    "* 奖励\n",
    "* 策略\n",
    "* 价值\n",
    "* 模型\n",
    "* 马尔科夫决策过程 \n",
    "\n",
    "\n",
    "## 3. 贝尔曼公式\n",
    "## 4. 贝尔曼最优公式\n",
    "## 5. model-base和model-free有什么区别 ，model具体指什么\n",
    "* model具体指什么？\n",
    "  1. model指的是智能体对环境运作规律的理解，具体来说就是智能体对环境的状态转移概率和奖励函数的理解。\n",
    "     * 状态转移概率：指的是智能体在当前状态 s 下采取 a 动作后，环境会转移到状态s‘ 的概率是多少。\n",
    "     * 奖励函数：指的是智能体在当前状态 s 采取 a 动作后，环境会给出多少奖励 r。 \n",
    "* model-base和model-free有什么区别？\n",
    "  1. model-base（基于模型）：\n",
    "    * 智能体知道环境如何运作（状态转移和奖励规则），可以直接用这些知识做规划。\n",
    "    * 比如下棋的时候，AI知道“移动皇后可以吃掉对方的棋子”（状态转移），也知道“将死对方国王能赢”（奖励）。它可以用这些规则提前推演未来几步，选择最优动作  \n",
    "    * model-base的缺点是需要知道环境的状态转移和奖励规则，这在现实中很难获得；模型必须准确，否则规划会出错（比如实际环境和你想象的模型不一致）。\n",
    "    * model-base的优点是可以直接用这些知识做规划，不需要考虑环境的随机性，数据效率高（不需要大量试错）。\n",
    "  2. model-free（无模型）：\n",
    "    * 智能体不知道环境如何运作，只能通过试错从经验中来学习。\n",
    "    * 比如学骑自行车时，一开始不知道“向左转车把会向左倾斜”（状态转移），也不知道“保持平衡会得到奖励”（奖励规则）。只能通过反复尝试，逐渐发现哪些动作会得到奖励，哪些动作会导致倾斜。\n",
    "    * model-free的缺点是需要大量试错，数据效率低（需要大量试错）。\n",
    "    * model-free的优点是不需要知道环境的状态转移和奖励规则，适用于复杂环境。\n",
    "## 6. 值迭代和策略迭代（动态规划、Model Base）\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
