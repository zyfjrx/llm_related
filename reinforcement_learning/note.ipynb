{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0867b97",
   "metadata": {},
   "source": [
    "## 一. 为什么关注强化学习\n",
    "* 强化学习得到的模型可以有超人类的表现。\n",
    "* 监督学习获取的监督数据，其实是人来标注的，比如 ImageNet 的图片的标签都是人类标注的。因此可以确定监督 学习算法的上限就是人类的表现，标注结果决定了它的表现永远不可能超越人类\n",
    "* 强化学习它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现，比如 DeepMind 的 AlphaGo 这样一个强化学习的算法可以把人类顶尖的棋手打败\n",
    "## 二. 强化学习几个关键概念\n",
    "* 状态\n",
    "* 动作\n",
    "* 奖励\n",
    "* 策略\n",
    "* 价值\n",
    "* 模型\n",
    "* 马尔科夫决策过程 \n",
    "\n",
    "\n",
    "## 三. 贝尔曼公式\n",
    "### 1. 贝尔曼解决什么问题？\n",
    "      * 在强化学习中我们需要评估一个状态的长期价值，比如：在一个迷宫游戏中，某个位置的价值 = 立刻能获得的奖励 + 未来能获得的奖励的折扣值\n",
    "      * 贝尔曼公式就是用来把这两个部分结合起来，计算状态的长期价值\n",
    "### 2. 贝尔曼公式的核心是：\n",
    "      * 贝尔曼公式是用来计算状态价值的，它是一个递归的公式，用来计算状态的长期价值。\n",
    "      * 贝尔曼公式的核心思想是：一个状态的价值等于它的即时奖励和未来奖励的期望。\n",
    "      * 贝尔曼公式的公式如下：\n",
    "        * V(s) = 即时奖励 + γ * 未来奖励的期望\n",
    "        * 其中，V(s) 表示状态 s 的长期，γ 是折扣因子（0 <= γ <= 1），表示未来奖励不如立刻奖励值钱。\n",
    "      * 即时奖励：指的是在状态 s 采取动作 a 后，立即获得的奖励，根据策略选动作，比如在状态s，我有50%的概率选择a1，50%的概率选择a2（由策略π(a∣s) 决定），执行动作后，环境会根据概率p(r∣s,a) 给我一个奖励（比如动作a1有80%概率得10分20%概率得5分，动作a2有50%概率得10分50%概率得5分）。把所有可能的动作和奖励组合起来，就可以得到即时奖励。\n",
    "\n",
    "        * 在状态s，如果动作a1的期望奖励是0.8*10 + 0.2*5 = 9分，动作a2的期望奖励是0.5*10 + 0.5*5 = 7.5分，则策略是50%选a1，50%选a2，那么在状态s的期望奖励就是9分*0.5 + 7.5分*0.5 = 8.25分。\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "## 四. 贝尔曼最优公式\n",
    "## 五. model-base和model-free有什么区别 ，model具体指什么\n",
    "* model具体指什么？\n",
    "  1. model指的是智能体对环境运作规律的理解，具体来说就是智能体对环境的状态转移概率和奖励函数的理解。\n",
    "     * 状态转移概率：指的是智能体在当前状态 s 下采取 a 动作后，环境会转移到状态s‘ 的概率是多少。\n",
    "     * 奖励函数：指的是智能体在当前状态 s 采取 a 动作后，环境会给出多少奖励 r。 \n",
    "* model-base和model-free有什么区别？\n",
    "  1. model-base（基于模型）：\n",
    "    * 智能体知道环境如何运作（状态转移和奖励规则），可以直接用这些知识做规划。\n",
    "    * 比如下棋的时候，AI知道“移动皇后可以吃掉对方的棋子”（状态转移），也知道“将死对方国王能赢”（奖励）。它可以用这些规则提前推演未来几步，选择最优动作  \n",
    "    * model-base的缺点是需要知道环境的状态转移和奖励规则，这在现实中很难获得；模型必须准确，否则规划会出错（比如实际环境和你想象的模型不一致）。\n",
    "    * model-base的优点是可以直接用这些知识做规划，不需要考虑环境的随机性，数据效率高（不需要大量试错）。\n",
    "  2. model-free（无模型）：\n",
    "    * 智能体不知道环境如何运作，只能通过试错从经验中来学习。\n",
    "    * 比如学骑自行车时，一开始不知道“向左转车把会向左倾斜”（状态转移），也不知道“保持平衡会得到奖励”（奖励规则）。只能通过反复尝试，逐渐发现哪些动作会得到奖励，哪些动作会导致倾斜。\n",
    "    * model-free的缺点是需要大量试错，数据效率低（需要大量试错）。\n",
    "    * model-free的优点是不需要知道环境的状态转移和奖励规则，适用于复杂环境。\n",
    "## 六. 值迭代和策略迭代（动态规划、Model Base）\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "72f218c9811fac1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:57:55.694495Z",
     "start_time": "2025-05-07T02:57:55.693107Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
