## 一. 为什么关注强化学习
* 强化学习得到的模型可以有超人类的表现。
* 监督学习获取的监督数据，其实是人来标注的，比如 ImageNet 的图片的标签都是人类标注的。因此可以确定监督 学习算法的上限就是人类的表现，标注结果决定了它的表现永远不可能超越人类
* 强化学习它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现，比如 DeepMind 的 AlphaGo 这样一个强化学习的算法可以把人类顶尖的棋手打败
## 二. 强化学习几个关键概念
* 状态
* 动作
* 奖励
* 策略
* 价值
* 模型
* 马尔科夫决策过程 


## 三. 贝尔曼公式
### 1. 贝尔曼解决什么问题？
  * 在强化学习中我们需要评估一个状态的长期价值，比如：在一个迷宫游戏中，某个位置的价值 = 立刻能获得的奖励 + 未来能获得的奖励的折扣值
  * 贝尔曼公式就是用来把这两个部分结合起来，计算状态的长期价值

### 2. 贝尔曼公式的核心是：
* 贝尔曼公式是用来计算状态价值的，它是一个递归的公式，用来计算状态的长期价值。
  * 贝尔曼公式的核心思想是：一个状态的价值等于它的即时奖励和未来奖励的期望。
  * 贝尔曼公式的公式如下：
    * ![公式](images/0.png)
    * V(s) = 即时奖励 + γ * 未来奖励的期望
    * 其中，V(s) 表示状态 s 的长期，γ 是折扣因子（0 <= γ <= 1），表示未来奖励不如立刻奖励值钱。
  * 即时奖励：指的是在状态 s 采取动作 a 后，立即获得的奖励，根据策略选动作，比如在状态s，我有50%的概率选择a1，50%的概率选择a2（由策略π(a∣s) 决定），执行动作后，环境会根据概率p(r∣s,a) 给我一个奖励（比如动作a1有80%概率得10分20%概率得5分，动作a2有50%概率得10分50%概率得5分）。把所有可能的动作和奖励组合起来，就可以得到即时奖励。
    * ![公式](images/1.png)
    * 在状态s，如果动作a1的期望奖励是0.8*10 + 0.2*5 = 9分，动作a2的期望奖励是0.5*10 + 0.5*5 = 7.5分，则策略是50%选a1，50%选a2，那么在状态s的期望奖励就是9分*0.5 + 7.5分*0.5 = 8.25分。
  * 未来奖励的期望：在状态s时未来所有奖励的期望值。
    * 转移到下一个状态：执行动作后环境会根据p(s′∣s,a) 转移到状态 s′
    * 递归计算价值：未来奖励的期望=所有可能的下一个状态s′的价值v(s′) 的加权平均。
    * 结合策略：策略影响动作选择，动作影响状态转移。 
    * ![公式](images/2.png)
    * 在状态s，如果动作a1有70%的概率转移到状态s1，30%的概率转移到状态s2，动作a2有50%概率转移到s1，50%转移到s2，且策略50%选择a1、50%选择a2，则未来奖励的期望是：0.5 *（0.7 * v（s1）+0.3 * v（s2））+0.5 *（0.5 * v（s1）+0.5 * v（s2））。
### state-value和action-value有什么区别？
  * state-value：在当前位置，按照当前策略行动，最终能获得的长期奖励。
  * action-value：在当前位置，先执行某个动作，再按照策略行动，最终能获得的长期奖励。
  *![公式](images/4.png)
  * state-value是状态的综合得分，告诉你在当前策略下，这个位置有多好；action-value是动作的具体得分，告诉你在当前状态下，做某个动作有多好。
  * state-value = action-value的加权平均（权重由策略决定）
  * action-value = 即时奖励 + 折扣因子 * 后续的state-value。

### 贝尔曼最优公式
   * 目标：找到最优策略π∗，使得在任意状态下，状态价值函数v∗(s)或q*(s,a) 最大。
   * 贝尔曼最优公式：
     * ![公式](images/3.png)

## 四. model-base和model-free有什么区别 ，model具体指什么
* model具体指什么？
  1. model指的是智能体对环境运作规律的理解，具体来说就是智能体对环境的状态转移概率和奖励函数的理解。
     * 状态转移概率：指的是智能体在当前状态 s 下采取 a 动作后，环境会转移到状态s‘ 的概率是多少。
     * 奖励函数：指的是智能体在当前状态 s 采取 a 动作后，环境会给出多少奖励 r。 
* model-base和model-free有什么区别？
  1. model-base（基于模型）：
    * 智能体知道环境如何运作（状态转移和奖励规则），可以直接用这些知识做规划。
    * 比如下棋的时候，AI知道“移动皇后可以吃掉对方的棋子”（状态转移），也知道“将死对方国王能赢”（奖励）。它可以用这些规则提前推演未来几步，选择最优动作  
    * model-base的缺点是需要知道环境的状态转移和奖励规则，这在现实中很难获得；模型必须准确，否则规划会出错（比如实际环境和你想象的模型不一致）。
    * model-base的优点是可以直接用这些知识做规划，不需要考虑环境的随机性，数据效率高（不需要大量试错）。
  2. model-free（无模型）：
    * 智能体不知道环境如何运作，只能通过试错从经验中来学习。
    * 比如学骑自行车时，一开始不知道“向左转车把会向左倾斜”（状态转移），也不知道“保持平衡会得到奖励”（奖励规则）。只能通过反复尝试，逐渐发现哪些动作会得到奖励，哪些动作会导致倾斜。
    * model-free的缺点是需要大量试错，数据效率低（需要大量试错）。
    * model-free的优点是不需要知道环境的状态转移和奖励规则，适用于复杂环境。
## 六. 值迭代和策略迭代（动态规划、Model Base）