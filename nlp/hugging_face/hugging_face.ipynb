{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T06:51:15.236346Z",
     "start_time": "2025-07-01T06:51:12.569838Z"
    }
   },
   "source": "from transformers import AutoTokenizer,AutoModel",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llamafactory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:51:57.560817Z",
     "start_time": "2025-07-01T06:51:56.856584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/zhangyf/llm/bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"/Users/zhangyf/llm/bert-base-chinese\")"
   ],
   "id": "3dd2125a7c267c58",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:36.034613Z",
     "start_time": "2025-07-01T02:02:36.031785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.tokenize(\"我爱自然语言处理\")\n",
    "print( tokens)"
   ],
   "id": "ceb85a580733713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '自', '然', '语', '言', '处', '理']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:37.857577Z",
     "start_time": "2025-07-01T02:02:37.855065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ],
   "id": "ddb2806d37ffd052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:39.490141Z",
     "start_time": "2025-07-01T02:02:39.487451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens)"
   ],
   "id": "f368f2c8d1a252dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '自', '然', '语', '言', '处', '理']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:40.898797Z",
     "start_time": "2025-07-01T02:02:40.896580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.encode(\"我爱自然语言处理\", max_length=20, truncation=True, padding=\"max_length\")\n",
    "print(ids)"
   ],
   "id": "dfdf0b4c20105aed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:42.298771Z",
     "start_time": "2025-07-01T02:02:42.295623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "string = tokenizer.decode(ids)\n",
    "print(string)"
   ],
   "id": "83d849d6062f4d1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 我 爱 自 然 语 言 处 理 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:02:44.495898Z",
     "start_time": "2025-07-01T02:02:44.492570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = tokenizer(\"我爱自然语言处理\",max_length=20,truncation=True,padding=\"max_length\")\n",
    "print(data)"
   ],
   "id": "bb179c92808a6021",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:52:18.681263Z",
     "start_time": "2025-07-01T06:52:18.677062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = [\"我爱自然语言处理\", \"我爱人工智能\", \"我们一起学习\"]\n",
    "encoded = tokenizer(texts, max_length=10, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(encoded)"
   ],
   "id": "c75db4595069383e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415,  102],\n",
      "        [ 101, 2769, 4263,  782, 2339, 3255, 5543,  102,    0,    0],\n",
      "        [ 101, 2769,  812,  671, 6629, 2110,  739,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:05:41.250540Z",
     "start_time": "2025-07-01T07:05:41.175382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **encoded\n",
    "    )\n",
    "print(outputs.keys())\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs.pooler_output.shape)\n",
    "print(outputs.pooler_output)\n",
    "print(outputs.last_hidden_state[:,0,:])"
   ],
   "id": "8abe5b7c4c9ca7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "torch.Size([3, 10, 768])\n",
      "torch.Size([3, 768])\n",
      "tensor([[ 0.9990,  0.9994,  0.9955,  ..., -0.9990, -0.9827,  0.9323],\n",
      "        [ 0.9972,  1.0000,  0.7786,  ..., -0.9976, -0.9421,  0.8830],\n",
      "        [ 0.9995,  1.0000,  0.9952,  ..., -0.9994, -0.9972,  0.9198]])\n",
      "tensor([[-0.3187, -0.1426, -0.2135,  ...,  0.2972,  0.2283, -0.2938],\n",
      "        [-0.2295,  0.1760, -0.2449,  ...,  0.1220,  0.4493,  0.1019],\n",
      "        [-0.1337,  0.3413,  0.1714,  ...,  0.6724,  0.1278,  0.0090]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "efb25efb7797b230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### datasets库",
   "id": "f6ba805879b16240"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:53:44.980666Z",
     "start_time": "2025-07-01T06:53:44.641361Z"
    }
   },
   "cell_type": "code",
   "source": "from datasets import load_dataset",
   "id": "6fe9e19bd70f7f3d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:08:25.233895Z",
     "start_time": "2025-07-01T07:08:24.358054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_dict = load_dataset(\"csv\", data_files=\"../review_analyze_bert/data/raw/online_shopping_10_cats.csv\")\n",
    "print(ds_dict)"
   ],
   "id": "a041b19d26ceba26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['cat', 'label', 'review'],\n",
      "        num_rows: 62774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:08:37.070675Z",
     "start_time": "2025-07-01T07:08:37.067523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datasets = ds_dict[\"train\"]\n",
    "print(train_datasets)"
   ],
   "id": "9a7acffe4622cfd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['cat', 'label', 'review'],\n",
      "    num_rows: 62774\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:08:39.161666Z",
     "start_time": "2025-07-01T07:08:39.158371Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_datasets[0])",
   "id": "448e2aa41d87c6a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': '书籍', 'label': 1, 'review': '做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:08:40.922070Z",
     "start_time": "2025-07-01T07:08:40.918742Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_datasets[:3])",
   "id": "390ffc2f55366ca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': ['书籍', '书籍', '书籍'], 'label': [1, 1, 1], 'review': ['做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。', '作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。', '作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？为什么在文化上有着深刻纽带关系的中国和日本却在经济发展上有着极大的差异？为什么英国的北美殖民地造就了经济强大的美国，而西班牙的北美殖民却造就了范后的墨西哥？……很有价值，但不包括【中国近代史专业】。']}\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:11:04.842937Z",
     "start_time": "2025-07-01T07:11:04.839821Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_datasets[0:3][\"review\"])",
   "id": "5c333714190a261f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。', '作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。', '作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？为什么在文化上有着深刻纽带关系的中国和日本却在经济发展上有着极大的差异？为什么英国的北美殖民地造就了经济强大的美国，而西班牙的北美殖民却造就了范后的墨西哥？……很有价值，但不包括【中国近代史专业】。']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:26:59.793258Z",
     "start_time": "2025-07-01T07:26:59.788879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 删除列\n",
    "train_datasets = train_datasets.remove_columns([\"cat\"])\n",
    "print(train_datasets)"
   ],
   "id": "84069a6dbb444cb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'review'],\n",
      "    num_rows: 62774\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T07:30:17.753721Z",
     "start_time": "2025-07-01T07:30:17.659825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 过滤\n",
    "train_datasets = train_datasets.filter(lambda x: x[\"review\"] is not None and x[\"review\"].strip() != \"\" and x[\"label\"] in [0,1])\n",
    "print(train_datasets)"
   ],
   "id": "f89c17a9f8af7071",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 62774/62774 [00:00<00:00, 705825.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'review'],\n",
      "    num_rows: 62773\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:17:50.390181Z",
     "start_time": "2025-07-01T08:17:50.376654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 划分数据集\n",
    "dataset_dict = train_datasets.train_test_split(test_size=0.2)\n",
    "print(dataset_dict[\"train\"])\n",
    "print(dataset_dict[\"test\"][0])"
   ],
   "id": "f9b5bf100a99379d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'review'],\n",
      "    num_rows: 50218\n",
      "})\n",
      "{'label': 1, 'review': '自己亲手装的。有工具不难装：还可以。家庭用买小了。送的花洒不怎么样。'}\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:01:31.298274Z",
     "start_time": "2025-07-01T08:01:22.599913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 编码数据\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/zhangyf/llm/bert-base-chinese\")\n",
    "# 逐个样本处理\n",
    "def encode_fn(example):\n",
    "    return tokenizer(example[\"review\"], truncation=True, padding=\"max_length\", max_length=10)\n",
    "\n",
    "dataset = dataset_dict[\"train\"].map(encode_fn, batched=False,remove_columns=[\"review\"])\n",
    "print(dataset)"
   ],
   "id": "66182865222d7a85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50218/50218 [00:08<00:00, 5790.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 50218\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:08:10.506021Z",
     "start_time": "2025-07-01T08:08:09.215958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_fn(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True, padding=\"max_length\", max_length=10)\n",
    "dataset = dataset_dict[\"train\"].map(batch_fn, batched=True,remove_columns=[\"review\"])\n",
    "print(dataset)"
   ],
   "id": "34f8c2a85744bf88",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50218/50218 [00:01<00:00, 39328.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 50218\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:19:38.010482Z",
     "start_time": "2025-07-01T08:19:36.314954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dict = dataset_dict.map(batch_fn,batched=True,remove_columns=[\"review\"])\n",
    "print(dataset_dict)"
   ],
   "id": "3b639771f80160da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50218/50218 [00:01<00:00, 38441.70 examples/s]\n",
      "Map: 100%|██████████| 12555/12555 [00:00<00:00, 33113.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50218\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 12555\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:20:35.498114Z",
     "start_time": "2025-07-01T08:20:35.460341Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_dict.save_to_disk(\"data\")",
   "id": "2db5190a44ad30c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50218/50218 [00:00<00:00, 2435389.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12555/12555 [00:00<00:00, 1988726.41 examples/s]\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:21:59.215406Z",
     "start_time": "2025-07-01T08:21:59.205712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_dict = load_from_disk(\"data\")\n",
    "print(dataset_dict)"
   ],
   "id": "1f75ee7630b22f15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50218\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 12555\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:23:32.652164Z",
     "start_time": "2025-07-01T08:23:30.649097Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_dict[\"train\"].to_csv(\"data/train.csv\")",
   "id": "aebd2be8fbb95cd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 51/51 [00:01<00:00, 25.57ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4923850"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:24:21.191530Z",
     "start_time": "2025-07-01T08:24:21.139721Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_dict[\"test\"].to_json(\"data/test.jsonl\")",
   "id": "2e4f47da0a8e70a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 285.07ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1887941"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:33:03.428554Z",
     "start_time": "2025-07-01T08:33:02.398693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/test.jsonl\")\n",
    "train_datasets = dataset[\"train\"]\n",
    "print(train_datasets)"
   ],
   "id": "ad0010a6e82e2f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 12555\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d3e9b605f68b349"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### pytorch DataLoader继承",
   "id": "19a8264d9bcbe6b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:34:23.272074Z",
     "start_time": "2025-07-01T08:34:23.268527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datasets.set_format(type=\"torch\")\n",
    "print(train_datasets)"
   ],
   "id": "4404e4a203375053",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 12555\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:35:33.440506Z",
     "start_time": "2025-07-01T08:35:33.433078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_datasets, batch_size=128, shuffle=True)\n",
    "for batch in train_loader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch[\"attention_mask\"].shape)\n",
    "    print(batch[\"token_type_ids\"].shape)\n",
    "    print(batch[\"label\"].shape)\n",
    "    break"
   ],
   "id": "194554e376bd60fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0eac992cbe60aa3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
